{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Incident Root Cause Analysis \n",
    "\n",
    "Incident Reports in ITOps usually states the symptoms. Identifying the root cause of the symptom quickly is a key determinant to reducing resolution times and improving user satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from the terminal\n",
    "#pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06.02. Preprocessing Incident Data\n",
    "\n",
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data file into a Pandas Dataframe\n",
    "symptom_data = pd.read_csv(\"root_cause_analysis.csv\")\n",
    "\n",
    "# Explore the data loaded\n",
    "print(symptom_data.dtypes)\n",
    "symptom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert  data\n",
    "\n",
    "Input data needs to be converted to formats that can be consumed by ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "symptom_data['ROOT_CAUSE'] = label_encoder.fit_transform(symptom_data['ROOT_CAUSE'])\n",
    "\n",
    "# Convert Pandas DataFrame to a numpy vector\n",
    "np_symptom = symptom_data.to_numpy().astype(float)\n",
    "\n",
    "# Extract the feature variables (X)\n",
    "X_data = np_symptom[:,1:8]\n",
    "\n",
    "# Extract the target variable (Y), conver to one-hot-encodign\n",
    "Y_data=np_symptom[:,8]\n",
    "Y_data = tf.keras.utils.to_categorical(Y_data,3)\n",
    "\n",
    "# Split training and test data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, test_size=0.10)\n",
    "\n",
    "print(\"Shape of feature variables :\", X_train.shape)\n",
    "print(\"Shape of target variable :\",Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.03. Building and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Setup Training Parameters\n",
    "EPOCHS=20\n",
    "BATCH_SIZE=64\n",
    "VERBOSE=1\n",
    "OUTPUT_CLASSES=len(label_encoder.classes_)\n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "# Create a Keras sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add a Dense Layer\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                             input_shape=(7,),\n",
    "                              name='Dense-Layer-1',\n",
    "                              activation='relu'))\n",
    "\n",
    "# Add a second dense layer\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                              name='Dense-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "# Add a softmax layer for categorial prediction\n",
    "model.add(keras.layers.Dense(OUTPUT_CLASSES,\n",
    "                             name='Final',\n",
    "                             activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Build the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "# Evaluate the model against the test dataset and print results\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06.04. Predicting Root Causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass individual flags to Predict the root cause\n",
    "import numpy as np\n",
    "\n",
    "CPU_LOAD=1\n",
    "MEMORY_LOAD=0\n",
    "DELAY=0\n",
    "ERROR_1000=0\n",
    "ERROR_1001=1\n",
    "ERROR_1002=1\n",
    "ERROR_1003=0\n",
    "\n",
    "prediction=np.argmax(model.predict(\n",
    "    [[CPU_LOAD,MEMORY_LOAD,DELAY,\n",
    "      ERROR_1000,ERROR_1001,ERROR_1002,ERROR_1003]]), axis=1 )\n",
    "\n",
    "print(label_encoder.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting as a Batch\n",
    "print(label_encoder.inverse_transform(np.argmax(\n",
    "        model.predict([[1,0,0,0,1,1,0],\n",
    "                                [0,1,1,1,0,0,0],\n",
    "                                [1,1,0,1,1,0,1],\n",
    "                                [0,0,0,0,0,1,0],\n",
    "                                [1,0,1,0,1,1,1]]), axis=1 )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
